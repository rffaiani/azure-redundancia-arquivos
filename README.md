#adf #azure #redundancia


# Criando processos de Redund√¢ncia de Arquivos no Azure

Vamos mapear a movimenta√ß√£o de dados de um banco de dados SQL on-premises para o **Azure Data Lake Storage**, organizando-os em camadas como _bronze_, _prata_ e _ouro_. 

---

### Vis√£o Geral:

1. **Fonte de Dados**: Banco de dados SQL on-premises.
2. **Destino**: Azure Data Lake Storage (camadas: _bronze_, _prata_, _ouro_).
3. **Ferramenta de Orquestra√ß√£o**: Azure Data Factory.
4. **Conectividade**: Integration Runtime auto-hospedado para acesso ao ambiente local.
   
---

### Etapas de Implementa√ß√£o

### 1. Criar uma Conta de Armazenamento no Azure

- Acesse o [Portal do Azure](https://portal.azure.com).

- Crie uma nova **Storage Account**.
   
- Dentro da conta de armazenamento, crie os containers:
    
    - `log`
        
    - `bronze`
        
    - `prata`
        
    - `ouro`
        

### 2. Configurar o Azure Data Factory

- No portal do Azure, crie um novo recurso do **Azure Data Factory**.

- Ap√≥s a cria√ß√£o, acesse o **Azure Data Factory Studio**.
   

### 3. Criar o Integration Runtime Auto-Hospedado

Para acessar dados on-premises, √© necess√°rio configurar um **Integration Runtime (IR)** auto-hospedado.

- No ADF Studio, v√° para a aba **Gerenciar**.

- Selecione **Runtimes de Integra√ß√£o** e clique em **+ Novo**.
   
- Escolha **Auto-Hospedado** e siga as instru√ß√µes para instala√ß√£o no servidor local.

![img1](./z_integration runtime.png)

üîó [Guia Oficial: Criar e Configurar um Integration Runtime Auto-Hospedado](https://learn.microsoft.com/pt-br/azure/data-factory/create-self-hosted-integration-runtime)

### 4. Criar Linked Services

**a) Fonte (SQL Server On-Premises)**

- V√° para a aba **Gerenciar** > **Linked Services** > **+ Novo**.

- Selecione **SQL Server**.
   
- Configure a conex√£o utilizando o IR auto-hospedado criado anteriormente.
   

**b) Destino (Azure Data Lake Storage)**

- Ainda em **Linked Services**, clique em **+ Novo**.
   
- Selecione **Azure Data Lake Storage Gen2**.
   
- Insira as credenciais e informa√ß√µes da conta de armazenamento criada.

![img2](./z_linked_service.png)

üîó [Guia Oficial: Criar Linked Services](https://learn.microsoft.com/pt-br/azure/data-factory/concepts-linked-services)

### 5. Criar Datasets

**a) Dataset de Entrada (SQL Server)**

- V√° para a aba **Autor** > **Datasets** > **+ Novo Dataset**.

- Escolha **SQL Server** como tipo de dados.

- Associe ao Linked Service do SQL Server criado anteriormente.

- Selecione a tabela de origem desejada.


**b) Dataset de Sa√≠da (Azure Data Lake Storage)**

- Crie um novo dataset selecionando **Azure Data Lake Storage Gen2**.

- Associe ao Linked Service do Data Lake.

- Defina o caminho do arquivo como `bronze/nome_do_arquivo.txt`.

![img3](./z_create_datasets.png)

üîó [Guia Oficial: Criar Datasets](https://learn.microsoft.com/pt-br/azure/data-factory/concepts-datasets-linked-services)

### 6. Criar o Pipeline de C√≥pia de Dados

- V√° para a aba **Autor** > **Pipelines** > **+ Novo Pipeline**.

- Nomeie o pipeline como `CopiaSQLparaBronze`.

- No painel de atividades, arraste a atividade **Copy Data** para o canvas.

- Configure:

    - **Fonte**: Dataset de entrada (SQL Server).

    - **Destino**: Dataset de sa√≠da (Data Lake - bronze).

- Salve e publique o pipeline.


üîó [Guia Oficial: Atividade de C√≥pia](https://learn.microsoft.com/pt-br/azure/data-factory/copy-activity-overview)

### 7. Executar e Monitorar o Pipeline

- Ap√≥s a publica√ß√£o, clique em **Trigger** > **Trigger Agora** para executar manualmente.

- V√° para a aba **Monitorar** para acompanhar a execu√ß√£o e verificar logs.


---

### Estrutura de Camadas no Data Lake

Organize os dados no Azure Data Lake Storage em camadas para melhor governan√ßa e processamento:

- **Bronze**: Dados brutos copiados diretamente da fonte.

- **Prata**: Dados limpos e transformados.

- **Ouro**: Dados prontos para consumo anal√≠tico e relat√≥rios.


Essa estrutura facilita a manuten√ß√£o, auditoria e escalabilidade do pipeline de dados.

---



